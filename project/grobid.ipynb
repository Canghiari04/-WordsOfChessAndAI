{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Obiettivo__\n",
    "\n",
    "Estrarre alcuni metadati relativi ai file PDF contenuti nella cartella _articlesGrobid_, mediante l'impiego di apposite librerie.\n",
    "\n",
    "Nuovamente, i dati estratti si suddividono in:\n",
    "- __DOI__, Digital Object Identifier, identificativo univoco di risorse digitali\n",
    "- __Title__, titolo dell'articolo scientifico\n",
    "- __Authors__, autore/autori partecipanti alla stesura del paper preso in considerazione\n",
    "- __Abstract__, piccolo riassunto del documento, privo di interpretazioni o critiche\n",
    "\n",
    "Il notebook ricalca lo stesso obiettivo disposto dal file __metadata.ipynb__, ma prendendo in esame un bacino molto più elevato rispetto ai dieci file contenuti in _articles_.\n",
    "Proprio per questa principale ragione, è stata adeguata una specifica libreria denominata __Grobid__, acronimo di _Generation Of Bibliographic Data_.\n",
    "\n",
    "_Grobid_ è una libreria di _machine learning_, il cui scopo consiste nell'estrazione e conversione di un file PDF in un _formato struttutato XML_. Le funzionalità sviluppate permettono di acquisire un insieme di dati già confezionato, disposti secondo l'ordine gerarchico espresso dal _markup language_ in questione. \n",
    "\n",
    "Infatti, come presentato negli snippet di codice successivi, è stata adeguata la libreria _BeatifulSoupe_ affinchè il risultato ottenuto da _Grobid_, memorizzato all'interno di un'apposita _directory_, fosse convertito in un formato idoneo al linguaggio di programmazione utilizzato."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"../papers/articles/\"\n",
    "\n",
    "input_dir = \"../articlesGrobid/\"\n",
    "output_dir = \"../TEI/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le due classi presentate sono utilizzate rispettivamente per:\n",
    "- __Author__, classe rappresentativa di tutti gli _autori_ degli articoli scientifici\n",
    "- __Metadata__, ciascun oggetto istanziato della classe rappresenta i _metadati_ ottenuti di ogni singolo file posto all'interno della cartella _articlesGrobid_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Author:\n",
    "    def __init__(self, forename: str, surname: str):\n",
    "        self.forename = forename\n",
    "        self.surname = surname\n",
    "\n",
    "    def to_unique(self) -> str | None:\n",
    "        return self.forename + \" \" + self.surname or None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "\n",
    "class Metadata:\n",
    "    def __init__(self, DOI: str, path: str, title: str, author: List[Author], abstract: str, introduction: str):\n",
    "        self.DOI = DOI\n",
    "        self.path = path\n",
    "        self.title = title\n",
    "        self.author = author\n",
    "        self.abstract = abstract\n",
    "        self.introduction = introduction\n",
    "\n",
    "    def get_dict(self) -> Dict[str, Dict[str, str | List[Author] | None]]:\n",
    "        return {\n",
    "            self.path: {\n",
    "                \"DOI\": self.DOI,\n",
    "                \"Title\": self.title,\n",
    "                \"Author\": [item.to_unique() for item in self.author],\n",
    "                \"Abstract\": self.abstract,\n",
    "                \"Introduction\": self.introduction\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Took a certain range of PDF from the papers to articlesGrobid\n",
    "def define_grobid_dir(main_path: str):\n",
    "    for path in os.listdir(main_path):\n",
    "        try:\n",
    "            year = int(path[:2])\n",
    "\n",
    "            if year == 0 and path.endswith(\".pdf\"):\n",
    "                shutil.copy(main_path + path, input_dir)\n",
    "        except ValueError as e:\n",
    "            continue\n",
    "\n",
    "if not os.path.exists(input_dir):\n",
    "    os.makedirs(input_dir)\n",
    "    define_grobid_dir(pdf_path)\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prima di eseguire lo snippet di codice seguente, è necessario abilitare la _Web API_ di _Grobid_. Ciò può avvenire in due modi, suddivisi in:  \n",
    "- Installazione di _Grobid_ localmente, in cui è richiesta la previa presenza del _JDK (Java Development Kit)_ e di _Gradle_\n",
    "- Utilizzo di _Docker_ tramite l'_image_ fornita dalla documentazione, necessaria per realizzare il _container_ associato, ossia l'istanza eseguibile della stessa immagine\n",
    "\n",
    "La scelta progettuale è ricaduta su _Docker_, data l'estrema semplicità garantita dal tool. Di seguito, è riportato il comando necessario per abilitare la _Web API_:\n",
    "<div align=\"center\">\n",
    "    <p><b>docker run --rm --init --ulimit core=0 -p 8070:8070 grobid/grobid:0.8.1</b></p>\n",
    "</div>\n",
    "\n",
    "Infine, attraverso un qualsiasi _browser_, è possibile accertarsi se il servizio operi correttamente, accedendo alla pagina dedicata alla _console_ di _Grobid_, esposta tramite l'_URL https://localhost:8070_. La risposta ricevuta dal servizio consiste in un insieme di file _XML_, i quali sono memorizzati all'interno della cartella _TEI_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GROBID server is up and running\n"
     ]
    }
   ],
   "source": [
    "from grobid_client.grobid_client import GrobidClient\n",
    "\n",
    "grobid_client = GrobidClient(config_path=\"../json/grobid/config.json\")\n",
    "grobid_client.process(\"processFulltextDocument\", input_dir, output=output_dir, n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_pdf_paths() -> List[str]:\n",
    "    path_files = []\n",
    "\n",
    "    for path in os.listdir(output_dir):\n",
    "        path_files.append(path)\n",
    "\n",
    "    return path_files\n",
    "\n",
    "xml_paths = define_pdf_paths()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Extracting content from XML, after defining a list of progressive tags\n",
    "def extract_content(soup: BeautifulSoup, tags: List[str]) -> str | None:\n",
    "    try:\n",
    "        element = soup.find(tags[0])\n",
    "\n",
    "        for tag in tags[1:]:\n",
    "            element = element.find(tag)\n",
    "\n",
    "        return element.contents[0]\n",
    "    except Exception:\n",
    "        return None\n",
    "    \n",
    "# Function used to detect authors' forename and username by RegEx\n",
    "def extract_name(expression: str, persName) -> str | None:\n",
    "    try:\n",
    "        match = re.search(expression, str(persName))\n",
    "\n",
    "        return match.group(1)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# BeatifulSoup consists in a data structure representing a parsed XML file\n",
    "def extract_field_from_xml(path_files: List[str]) -> List[Metadata]:\n",
    "    list_authors: List[Author] = []\n",
    "    list_metadata: List[Metadata] = []\n",
    "    \n",
    "    for path in sorted(path_files):\n",
    "        try: \n",
    "            if path.endswith(\".xml\"):\n",
    "                with open(output_dir + path, \"r\") as content:\n",
    "                    xml = content.read()\n",
    "\n",
    "                soup = BeautifulSoup(xml, \"xml\")\n",
    "                \n",
    "                title = extract_content(soup, [\"titleStmt\", \"title\"])\n",
    "                abstract = extract_content(soup, [\"profileDesc\", \"abstract\", \"p\"])\n",
    "        \n",
    "                try:\n",
    "                    introduction = extract_content(soup, [\"body\", \"div\", \"p\"])\n",
    "                except Exception:\n",
    "                    introduction = None\n",
    "\n",
    "                try: \n",
    "                    persNames = soup.find(\"sourceDesc\").find(\"biblStruct\").find(\"analytic\").find_all(\"persName\")\n",
    "                except Exception:\n",
    "                    persNames = None\n",
    "\n",
    "                if persNames is not None:\n",
    "                    for persName in persNames:\n",
    "                        forename = extract_name(r\"<forename\\stype=\\\"first\\\">(.*?)<\\/forename>\", persName)\n",
    "                        surname = extract_name(r\"<surname>(.*?)<\\/surname>\", persName)\n",
    "                        \n",
    "                        if None not in (forename, surname):\n",
    "                            author = Author(forename, surname)\n",
    "                            list_authors.append(author)\n",
    "\n",
    "                list_metadata.append(Metadata(DOI=None, path=path, title=title, author=list_authors, abstract=abstract, introduction=introduction))\n",
    "                list_authors = []\n",
    "                \n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(\"Error during parsing \", path,\" file: \", e)\n",
    "\n",
    "    return list_metadata\n",
    "\n",
    "list_metadata = extract_field_from_xml(xml_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nonostante l'applicazione di _Grobid_, alcuni PDF sono privi di _metadati_. Pertanto, affinchè l'analisi condotta possa essere estesa ad un numero sempre più vasto di file,\n",
    "è stata implementata la libreria _pypdf_; quest'ultima, permette di manipolare ogni singola pagina che componga il PDF. A tal proposito, sono acquisite ulteriori informazioni disponibili tramite il _field metadata_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "parsing for Object Streams\n",
      "parsing for Object Streams\n",
      "parsing for Object Streams\n",
      "parsing for Object Streams\n",
      "parsing for Object Streams\n"
     ]
    }
   ],
   "source": [
    "from pypdf import PdfReader\n",
    "\n",
    "def define_authors(str_author: str) -> List[Author]:\n",
    "    list_authors: List[Metadata] = []\n",
    "    try:\n",
    "        authors = re.split(r\"\\sand\\s\", str_author)\n",
    "\n",
    "        for author in authors:\n",
    "            items = author.split(\" \")\n",
    "            list_authors.append(Author(items[0], items[1]))\n",
    "\n",
    "        return list_authors\n",
    "    except Exception:\n",
    "        return list_authors\n",
    "\n",
    "def retrieve_missing_metadata(list_metadata: List[Metadata]):\n",
    "    for metadata in list_metadata:\n",
    "        if metadata.title is None or len(metadata.author) == 0:\n",
    "            pdf_path = re.search(r\"(^.*)?.grobid\", metadata.path).group(1) + \".pdf\"\n",
    "\n",
    "            _pdfreader = PdfReader(input_dir + pdf_path).metadata\n",
    "            \n",
    "            title = _pdfreader.title or None\n",
    "            author = _pdfreader.author or []\n",
    "\n",
    "            metadata.title = title\n",
    "            metadata.path = pdf_path\n",
    "            metadata.author = define_authors(author)\n",
    "\n",
    "retrieve_missing_metadata(list_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "\n",
    "from crossref.restful import Works\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "def get_authors_string(list_authors: List[Author]) -> List[str]:\n",
    "    str_authors = \"\"\n",
    "    for i in range(0, len(list_authors)):\n",
    "        if i >= len(list_authors):\n",
    "            str_authors += list_authors[i].forename + \" \" + list_authors[i].surname\n",
    "        else:\n",
    "            str_authors += list_authors[i].forename + \" \" + list_authors[i].surname + \" \"\n",
    "\n",
    "    return str_authors\n",
    "\n",
    "def similar(str_1: str, str_2: str) -> float:\n",
    "    return SequenceMatcher(None, str_1, str_2).ratio()\n",
    "\n",
    "work = Works()\n",
    "\n",
    "def send_crossref_request(title: str, list_authors: List[str]) -> str:\n",
    "    url_request = work.query(bibliographic=title, author=get_authors_string(list_authors)).url\n",
    "\n",
    "    time.sleep(2)\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url_request)\n",
    "\n",
    "        match response.status_code:\n",
    "            case 200:\n",
    "                content = response.json()\n",
    "                message = content.get(\"message\")\n",
    "\n",
    "                items = message.get(\"items\")\n",
    "                for item in items:\n",
    "                    if similar(title, item[\"title\"][0]) > 0.5:\n",
    "                        return item[\"DOI\"]\n",
    "                                                                \n",
    "                    continue\n",
    "            case 400:\n",
    "                raise Exception(\"Error during request to CrossRef REST API: Bad Request\")\n",
    "            case _:\n",
    "                raise Exception(\"Error during request to CrossRef REST API: WTF\")\n",
    "    except Exception as e:\n",
    "        print(\"Error during request to CrossRef REST API:\", e)\n",
    "\n",
    "for metadata in list_metadata:\n",
    "    if None in (metadata.title, metadata.author):\n",
    "        print(metadata.path)\n",
    "    else:\n",
    "        doi = send_crossref_request(metadata.title, metadata.author)\n",
    "\n",
    "        if doi is not None:\n",
    "            metadata.DOI = doi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_not_found_dois = list(filter(lambda metadata: metadata.DOI is None, list_metadata))\n",
    "list_found_dois = list(filter(lambda metadata: metadata.DOI is not None, list_metadata))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La sezione finale del notebook si concentra su alcuni controlli attuati per accertarsi della __bontà__ dei _Digital Object Identifier_ ricavati. Ciò avviene interrogando nuovamente la _REST API_ di _CrossRef_. Tuttavia, in questa casistica, non sono combinati il _title_ e gli _authors_ estrapolati precedentemente, attraverso apposite librerie, ma è inserito all'interno dell'_URL_ lo stesso _DOI_; in questo modo, è possibile definire il grado di similarità tra le informazioni già in possesso rispetto ai nuovi dati ottenuti dalla risposta dell'_Application Programming Interface_.\n",
    "\n",
    "Di seguito, sono riportate le operazioni principali del comportamento descritto, suddivise in:\n",
    "- __Definizione della lista__, lista contenente l'insieme di tutti i _metadati_ riferiti ai PDF contenuti nella _directory articlesGrobid_\n",
    "- __Invio della richiesta__, richiesta riferita alla _REST API_ di _CrossRef_, da cui, qualora sia positiva la risposta, saranno ricavati i \"nuovi\" dati necessari per il confronto\n",
    "- __Definizione grado di similarità__, tramite la libreria _SequenceMatcher_ è estrapolato il _grado di similarità_ del _metadato_ analizzato, qualora dovesse essere maggiore di _0.6_, dato che si tratta di un valore decimale appartenente all'intervallo _[0, 1]_, è rimosso dalla lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small check to detect the correctness of metadata\n",
    "def define_wrong_dois(list_metadata: List[Metadata]) -> List[Metadata]:\n",
    "    # List that will contain the \"wrong\" DOIs retrieved previously\n",
    "    list_wrong_dois = list(filter(lambda metadata: metadata.DOI is not None, list_metadata))\n",
    "\n",
    "    for metadata in list_metadata:\n",
    "        if metadata.DOI is not None:\n",
    "            url_request = work.query().url + \"/\" + metadata.DOI\n",
    "        \n",
    "            try:\n",
    "                response = requests.get(url_request)\n",
    "\n",
    "                match response.status_code:\n",
    "                    case 200:\n",
    "                        content = response.json()\n",
    "\n",
    "                        try:\n",
    "                            message = content.get(\"message\")\n",
    "\n",
    "                            # Another check may concern the authors and the publication date\n",
    "                            titles = message.get(\"title\")\n",
    "                            authors = message.get(\"author\")\n",
    "                \n",
    "                            for title in titles:\n",
    "                                if similar(metadata.title, title) > 0.6:\n",
    "                                    list_wrong_dois.remove(metadata)\n",
    "                                    break\n",
    "\n",
    "                                continue\n",
    "\n",
    "                        except Exception as e:\n",
    "                            print(\"Field not found\", metadata.path)\n",
    "                            continue\n",
    "                    case 400:\n",
    "                        raise Exception(\"Error during request to CrossRef REST API: Bad Request\")\n",
    "                    case _:\n",
    "                        raise Exception(\"Error during request to CrossRef REST API: WTF\")\n",
    "            except Exception as e:\n",
    "                print(\"Error during request to CrossRef REST API:\", e)\n",
    "\n",
    "    return list_wrong_dois\n",
    "\n",
    "list_wrong_dois = define_wrong_dois(list_metadata)\n",
    "list_correct_dois = list(filter(lambda metadata: metadata in list_found_dois and metadata not in list_wrong_dois, list_metadata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def define_list(_list: List[Metadata]) -> List:\n",
    "    list_json = []\n",
    "\n",
    "    for item in _list:\n",
    "        list_json.append(item.get_dict())\n",
    "\n",
    "    return list_json\n",
    "\n",
    "with open(\"../json/grobid/summary.json\", \"w\") as file:\n",
    "    json.dump(define_list(list_metadata), file, indent=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># DOI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Total</th>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Found</th>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Not found</th>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           # DOI\n",
       "Total         76\n",
       "Found         46\n",
       "Not found     30"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># DOI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Found</th>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Correct</th>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wrong</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         # DOI\n",
       "Found       46\n",
       "Correct     37\n",
       "Wrong        9"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas \n",
    "\n",
    "dict_doi_summary = {\n",
    "    \"Total\": len(list_metadata),\n",
    "    \"Found\": len(list_found_dois),\n",
    "    \"Not found\": len(list_not_found_dois)\n",
    "}\n",
    "\n",
    "dict_doi_accuracy = {\n",
    "    \"Found\": len(list_found_dois),\n",
    "    \"Correct\": len(list_correct_dois),\n",
    "    \"Wrong\": len(list_wrong_dois)\n",
    "}\n",
    "\n",
    "df_doi_summary = pandas.DataFrame.from_dict(dict_doi_summary, orient=\"index\", columns=[\"# DOI\"])\n",
    "display(df_doi_summary)\n",
    "\n",
    "df_doi_accuracy = pandas.DataFrame.from_dict(dict_doi_accuracy, orient=\"index\", columns=[\"# DOI\"])\n",
    "display(df_doi_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
