{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Obiettivo__\n",
    "\n",
    "Migliorare l'estrazione delle sezioni che compongono il PDF, concentrandosi soprattutto su _Abstract_ ed _Introduction_, su cui saranno applicate tecniche per estrapolare le _keywork_. Pertanto occorre ripetere l'analisi affrontata nel file _metadata.ipynb_, ma escludendo alcune sezioni e concentrandosi solamente su alcune di esse.\n",
    "\n",
    "Potrebbe essere adeguato nuovamente l'approccio delineato in _metadata.ipynb_, dove è individuato l'indice della linea, successivamente ad un'azione di _splitlines_, in cui compaia la _keyword_.\n",
    "\n",
    "La pipeline si suddivide in:\n",
    "- __Regex__, individuare i pattern principali per riconoscere alcune parole / frasi ripetitive nel testo\n",
    "- __Estrazione__, estrapolazione della sezione di testo coordinata rispetto allo _start index_ ed _end index_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"../articles/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScannedText:\n",
    "    def __init__(self, start_index, end_index, text):\n",
    "        self.start_index = start_index\n",
    "        self.end_index = end_index\n",
    "        self.text = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from typing import List\n",
    "\n",
    "def get_path_pdf_files(pdf_path: str) -> List[str]:\n",
    "    path_files = []\n",
    "\n",
    "    for path in os.listdir(pdf_path):\n",
    "        path_files.append(pdf_path + path)\n",
    "    \n",
    "    return path_files\n",
    "\n",
    "path_pdf_files = get_path_pdf_files(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytesseract\n",
    "\n",
    "from typing import Dict\n",
    "from pdf2image import convert_from_path\n",
    "\n",
    "# It will convert all the pages to images\n",
    "def convert_pdf_to_images(path: str) -> list:\n",
    "    try:\n",
    "        return convert_from_path(path)\n",
    "    except Exception as e:\n",
    "        print(\"Error occur during conversion from pdf to image:\", e)\n",
    "        return None\n",
    "\n",
    "def convert_pdf_to_text(paths: list[str]) -> Dict[str, str]:\n",
    "    _dict: Dict[str, str] = {}\n",
    "\n",
    "    for path in paths:\n",
    "        images = convert_pdf_to_images(path)    \n",
    "        \n",
    "        text = \"\"\n",
    "        for image in images:\n",
    "            try:\n",
    "                text = text + pytesseract.image_to_string(image)\n",
    "            except Exception as e:\n",
    "                print(\"Error occur during conversion from image to string:\", e)\n",
    "        \n",
    "        _dict[path] = text\n",
    "\n",
    "    return _dict\n",
    "\n",
    "dict_scanned_text = convert_pdf_to_text(path_pdf_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fino ad ora è stato riportato lo stesso codice utilizzato in _metadata.ipynb_ per la conversione del file PDF in _str_, affinchè sia possibile attuare tecniche di estrazione del testo con maggiore facilità, oltre alla compatibilità garantita mediante l'impiego di un linguaggio di programmazione.\n",
    "\n",
    "_Nota bene_: nella funzione *convert_pdf_to_images* è applicata la conversione di ciascuna pagina che componga il PDF, a differenza di _metadata.ipynb_ in cui sono convertite solamente le facciate iniziali dato che l'obiettivo ricade nella definizione dei metadati di ogni file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from typing import Dict\n",
    "from itertools import islice\n",
    "\n",
    "def get_target_line_index(expression: str, text: str) -> int:\n",
    "    count_line = 0\n",
    "    lines = text.splitlines()\n",
    "\n",
    "    for line in lines:\n",
    "        if len(re.findall(expression, line.lower())) > 0:\n",
    "            return count_line\n",
    "        \n",
    "        count_line += 1\n",
    "\n",
    "    return -1\n",
    "\n",
    "def remove_references_section(expression: str, dict_scanned_text: Dict[str, str]) -> Dict[str, str]:\n",
    "    _dict: Dict[str, str] = {}\n",
    "\n",
    "    for key, value in dict_scanned_text.items():\n",
    "        index = get_target_line_index(expression, value.lower())\n",
    "\n",
    "        if index > -1:\n",
    "            lines = value.splitlines()\n",
    "\n",
    "            text = \"\"\n",
    "            for line in islice(lines, 0, index):\n",
    "                text += line + \"\\n\"\n",
    "                _dict[key] = text\n",
    "\n",
    "    return _dict\n",
    "\n",
    "dict_scanned_text_without_references = remove_references_section(r\"^[0-9]?.?\\s*\\breferences\\b$\", dict_scanned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sembra che non abbiano la sezione _References_ i seguenti file:\n",
    "- _83CondonThompson_\n",
    "- _07Beal_\n",
    "- _76Panek_\n",
    "\n",
    "Dopo un breve controllo, i file stessi non possiedono la sezione al loro interno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../articles/16DavidNetanyahuWolf.pdf: (start_21, end_35)\n",
      "../articles/90GeorgeSchaeffer.pdf: (start_10, end_30)\n",
      "../articles/91FeldmannMysliwietzMonien.pdf: (start_8, end_20)\n",
      "../articles/09CiancariniFavini 1.pdf: (start_7, end_40)\n",
      "../articles/ICGA_J_34_2_HHB_Zugzwangs_in_Chess_Studies.pdf: (start_41, end_51)\n",
      "../articles/19Kamlish.pdf: (start_10, end_39)\n",
      "../articles/96Brockington.pdf: (start_-1, end_5)\n",
      "\n",
      "\n",
      "../articles/16DavidNetanyahuWolf.pdf: (start_35, end_58)\n",
      "../articles/90GeorgeSchaeffer.pdf: (start_30, end_99)\n",
      "../articles/91FeldmannMysliwietzMonien.pdf: (start_20, end_105)\n",
      "../articles/09CiancariniFavini 1.pdf: (start_40, end_99)\n",
      "../articles/ICGA_J_34_2_HHB_Zugzwangs_in_Chess_Studies.pdf: (start_51, end_101)\n",
      "../articles/19Kamlish.pdf: (start_39, end_603)\n",
      "../articles/96Brockington.pdf: (start_5, end_30)\n"
     ]
    }
   ],
   "source": [
    "def get_section_scanned_text(start_word:str, end_word:str, dict_scanned_text: dict[str, str]) -> Dict[str, ScannedText]:\n",
    "    _dict : Dict[str, ScannedText] = {}\n",
    "\n",
    "    for key, value in dict_scanned_text.items():\n",
    "        _dict[key] = ScannedText(get_target_line_index(start_word, value), get_target_line_index(end_word, value), value)\n",
    "\n",
    "    return _dict\n",
    "\n",
    "dict_scanned_abstract = get_section_scanned_text(r\"^abstract\", r\"1?\\.?\\s*introduction\", dict_scanned_text_without_references)\n",
    "dict_scanned_introduction = get_section_scanned_text(r\"^1?\\.?\\s*introduction\", r\"^2.?\\W\", dict_scanned_text_without_references)\n",
    "\n",
    "def stamp_found_indexes(_dict: Dict[str, ScannedText]):\n",
    "    for key, value in _dict.items():\n",
    "        print(key + \": \" + \"(start_\" + str(value.start_index) + \", end_\" +  str(value.end_index) + \")\")\n",
    "\n",
    "stamp_found_indexes(dict_scanned_abstract)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "stamp_found_indexes(dict_scanned_introduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Check if the found lines are correct according to the extraction\n",
    "for key in dict_scanned_text_without_references.keys():\n",
    "    with open(\"../txt/with.txt\", \"w\") as file:\n",
    "        file.write(dict_scanned_text[key])\n",
    "\n",
    "    time.sleep(15)\n",
    "\n",
    "    with open(\"../txt/without.txt\", \"w\") as file:\n",
    "        file.write(dict_scanned_text_without_references[key])\n",
    "\n",
    "    time.sleep(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rispetto alla keyword _1. Introduction_, alcuni file presentano delle limitazioni, quali:\n",
    "- _83CondonThompson_, scan scadente\n",
    "- _76Panek_, assente la sezione _Introduction_\n",
    "- _19Kamlish_, disposizione del documento in due colonne distinte, come già anticipato, rende più ardua la conversione da _images_ a _text_, poichè sovrappone il contenuto disposto tra le due colonne. Ciò avviene anche per il file _09CiancariniFavini_."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
