{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Obiettivo__\n",
    "\n",
    "Estrarre alcuni metadati relativi ai file PDF contenuti nella cartella _articles_, mediante l'impiego di apposite librerie per la gestione e manipolazione di file PDF.\n",
    "\n",
    "I dati estratti sono suddivisi in:\n",
    "- __DOI__, Digital Object Identifier, identificativo univoco di risorse digitali\n",
    "- __Title__, titolo dell'articolo / paper scientifico \n",
    "- __Author__, autore / autori partecipanti alla stesura dell'articolo preso in considerazione\n",
    "- __Abstract__, riassunto di un documento, privo di interpretazioni o critiche\n",
    "\n",
    "_DOI_ è l'acronimo di __Digital Object Identifier__, identificativo univoco di risorse digitali. Per poter apprendere tale informazione è necessario utilizzare la _REST API_ fornita da __CrossRef__; CrossRef è un'infrastuttura digitale dedita alle memorizzazione di tutti gli articoli posti in ambito accademico. Di seguito, si percepisce l'importanza del titolo e dell'autore per ogni PDF, affinchè interrogando la _Application Programming Interface_ sia possibile ottenere il DOI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_word = \"abstract\"\n",
    "pdf_path = \"../articles/\"\n",
    "crossref_url = \"https://api.crossref.org/works/10.1037/0003-066X.59.1.29/agency\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le tre classi riportate sono utilizzate rispettivamente per:\n",
    "- __ScannedText__, contiene sezioni del file PDF scannerizzato, dove _index_ rappresenta l'indice della linea in cui compare la _key word_ ricercata, mentre _text_ è una variabile _str_ utilizzata per memorizzare la scansione da immagine a stringa effettuata tramite _pytesseract_\n",
    "- __Metadata__, ciascun oggetto istanziato della classe rappresenta i metadati ottenuti di ogni singolo file presente all'interno della cartella _articles_\n",
    "- __AgentExtractor__, implementata per realizzare una __chain__ secondo le direttive della libreria __langchain__. Utilizzata per poter estrarre il _titolo_ e gli _autori_ degli articoli privi di _metadati_ già presenti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScannedText:\n",
    "    def __init__(self, index, text):\n",
    "        self.index = index\n",
    "        self.text = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "class Metadata:\n",
    "    def __init__(self, DOI=None, path=None, title=None, author=None, abstract=None):\n",
    "        self.DOI = DOI\n",
    "        self.path = path\n",
    "        self.title = title\n",
    "        self.author = author\n",
    "        self.abstract = abstract\n",
    "\n",
    "    def get_dict(self) -> Dict[str, Dict[str, str]]:\n",
    "        return {\n",
    "            self.path: {\n",
    "                \"DOI\": self.DOI,\n",
    "                \"Title\": self.title,\n",
    "                \"Author\": self.author,\n",
    "                \"Abstract\": self.abstract\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "class AgentExtractor:\n",
    "    def __init__(self):\n",
    "        system_message = \"\"\"\n",
    "            You are an assistant in charge to extract the title and the authors' list of scientific paper.\n",
    "\n",
    "            You must extract at least the title and the authors, any other information is not requested. Return all the\n",
    "            fields required like an unique string; all the fields must be separated by a comma.\n",
    "\n",
    "            Example of an extraction:\n",
    "            Title, First Author, Second Author, ..., Last Author\n",
    "        \"\"\"\n",
    "\n",
    "        prompt = ChatPromptTemplate(\n",
    "            [\n",
    "                (\"system\", system_message),\n",
    "                (\"human\", \"Scientific paper, which will be used to extract the title and the authors' list. \\n {text}\")\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        llm = ChatOpenAI(\n",
    "            model=\"gpt-4o\",\n",
    "            api_key=os.getenv(\"OPENAI_KEY\"),\n",
    "            temperature=0\n",
    "        )\n",
    "\n",
    "        self.agent = prompt | llm\n",
    "\n",
    "    def get_agent(self):\n",
    "        return self.agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La pipeline è composta da due fasi principali:\n",
    "- _Estrazione_ dei metadati\n",
    "- _Elaborazione_ dei metadati ottenuti\n",
    "\n",
    "Le librerie utilizzate per l'acquisizione dei metadati si fondano a loro volta sulla libreria __pdfMiner__, pertanto riescono ad estrapolare le informazioni circoscritte dagli stessi file. Perciò tutte le librerie trovate si equivalgono.\n",
    "\n",
    "Tramite la funzione _filter_, associata al metodo _query_ fornita dalla libreria __crossref__, è possibile recuperare l'_abstract_ degli articoli presenti all'interno dell'API. Tuttavia, è stato notato che l'impiego del filtro comporta ad una risposta completamente differente rispetto alla casistica in cui sia assente. Di seguito, è presentato lo snippet di codice implementato:\n",
    "\n",
    "<div align=\"center\">\n",
    "    <p><b>url_request = work.query(bibliographic=title, author=author).filter(has_abstract=true).url</b></p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def get_path_pdf_files(pdf_path: str) -> List[str]:\n",
    "    path_files = []\n",
    "\n",
    "    for path in os.listdir(pdf_path):\n",
    "        path_files.append(pdf_path + path)\n",
    "    \n",
    "    return path_files\n",
    "\n",
    "path_pdf_files = get_path_pdf_files(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytesseract\n",
    "\n",
    "from pdf2image import convert_from_path\n",
    "\n",
    "def convert_file_to_images(path: str) -> list:\n",
    "    try:\n",
    "        return convert_from_path(path)[:3]\n",
    "    except Exception as e:\n",
    "        print(\"Error during conversion from file to image:\", e)\n",
    "\n",
    "def scan_file_to_text(paths: list[str]) -> dict[str, str]:\n",
    "    dict_texts: Dict[str, str] = {}\n",
    "\n",
    "    for path in paths:\n",
    "        images = convert_file_to_images(path)\n",
    "\n",
    "        text = \"\"\n",
    "        try:\n",
    "            for image in images:\n",
    "                text = text + pytesseract.image_to_string(image)\n",
    "\n",
    "            dict_texts[path] = text\n",
    "        except Exception as e:\n",
    "            print(\"Error during conversion from image to text:\", e) \n",
    "        \n",
    "    return dict_texts\n",
    "\n",
    "scanned_text = scan_file_to_text(path_pdf_files)\n",
    "\n",
    "def get_target_word_index(keyword: str, text: str) -> int:\n",
    "    count_line = 0\n",
    "\n",
    "    lines = text.splitlines()\n",
    "    for line in lines:\n",
    "        if keyword in line:\n",
    "            return count_line\n",
    "        \n",
    "        count_line += 1\n",
    "    \n",
    "    return -1\n",
    "\n",
    "def detect_target_line(target_word:str, dict: dict[str, str]) -> dict[str, ScannedText]:\n",
    "    dict_lines: Dict[str, ScannedText] = {}\n",
    "\n",
    "    for key, value in dict.items():\n",
    "        dict_lines[key] = ScannedText(get_target_word_index(target_word, value.lower()), value)\n",
    "\n",
    "    return dict_lines\n",
    "\n",
    "abstracts = detect_target_line(\"abstract\", scanned_text)\n",
    "introductions = detect_target_line(\"introduction\", scanned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] Ignoring wrong pointing object 2 65536 (offset 0)\n",
      "[WARNING] Ignoring wrong pointing object 16 65536 (offset 0)\n",
      "[WARNING] Ignoring wrong pointing object 49 65536 (offset 0)\n",
      "[WARNING] Ignoring wrong pointing object 59 65536 (offset 0)\n",
      "[WARNING] Ignoring wrong pointing object 63 65536 (offset 0)\n",
      "[WARNING] Ignoring wrong pointing object 72 65536 (offset 0)\n",
      "[WARNING] Ignoring wrong pointing object 76 65536 (offset 0)\n",
      "[WARNING] Ignoring wrong pointing object 86 65536 (offset 0)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pymupdf\n",
    "import pdfplumber\n",
    "\n",
    "from pypdf import PdfReader\n",
    "\n",
    "def get_title_from_dicts(pdfreader: str, pymupdf: dict[str, str], pdfplumber: dict[str, str]) -> str:\n",
    "    return (pdfreader or pymupdf.get(\"title\") or pdfplumber.get(\"title\") or \"Not found\")\n",
    "\n",
    "def get_author_from_dicts(pdfreader: str, pymupdf: dict[str, str], pdfplumber: dict[str, str]) -> str:\n",
    "    return (pdfreader or pymupdf.get(\"author\") or pdfplumber.get(\"author\") or \"Not found\")\n",
    "\n",
    "def extract_title_and_author(i: int, len: int, paths: list[str], list_metadata: List[Metadata] = []) -> List[Metadata]:\n",
    "    if len == 0:\n",
    "        return list_metadata\n",
    "    else:\n",
    "        _pdfreader = PdfReader(paths[i]).metadata\n",
    "        _pymupdf = pymupdf.open(paths[i]).metadata\n",
    "        _pdfplumber = pdfplumber.open(paths[i]).metadata\n",
    "\n",
    "        title = get_title_from_dicts(_pdfreader.title, _pymupdf, _pdfplumber)\n",
    "        author = get_author_from_dicts(_pdfreader.author, _pymupdf, _pdfplumber)\n",
    "\n",
    "        list_metadata.append(Metadata(path=paths[i], title=title, author=author))\n",
    "\n",
    "        extract_title_and_author(i + 1, len - 1, paths)\n",
    "        return list_metadata\n",
    "\n",
    "list_metadata = extract_title_and_author(0, len(path_pdf_files), path_pdf_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Piccolo catalogo di articoli a cui manca anche il titolo e autore:\n",
    "- 09CiancariniFavini\n",
    "- 16DavidNetanyahuWolf\n",
    "- 19Kamlish\n",
    "- ICGA_J_34_2_HHB_Zugzwangs_in_Chess_Studies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "def get_authors_name(i: int, len:int, authors: list[str], field = \"\") -> str:\n",
    "    if len == 1:\n",
    "        return field + authors[i]\n",
    "    else:\n",
    "        return get_authors_name(i + 1, len - 1, authors, field + authors[i] + \" and \")\n",
    "    \n",
    "extractor = AgentExtractor()\n",
    "agent_extractor = extractor.get_agent()\n",
    "\n",
    "for metadata in list_metadata:\n",
    "    if \"Not found\" in (metadata.title, metadata.author):\n",
    "        metadata.title = \"\"\n",
    "        metadata.author = \"\"\n",
    "\n",
    "        answer = agent_extractor.invoke(scanned_text.get(metadata.path)).content\n",
    "        \n",
    "        fields = answer.split(\", \")\n",
    "\n",
    "        metadata.title = fields[0]\n",
    "        metadata.author = get_authors_name(0, len(fields[1:]), fields[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def remove_white_spaces(index: int, text: str) -> List[str]:\n",
    "    lines = text.splitlines()\n",
    "    first_part = lines[:index + 1]\n",
    "\n",
    "    start_index = index + 1\n",
    "    count_line = start_index\n",
    "\n",
    "    for line in lines[start_index:]:\n",
    "        if len(line) == 0:\n",
    "            count_line += 1\n",
    "            continue\n",
    "        else:\n",
    "            break\n",
    "        \n",
    "    second_part = lines[count_line:]\n",
    "    \n",
    "    return first_part + second_part\n",
    "\n",
    "def extract_section_lines(index: int, lines: list[str]) -> str:\n",
    "    abstract = \"\"\n",
    "\n",
    "    for line in lines[index:]:\n",
    "        if len(line) == 0:\n",
    "            break\n",
    "\n",
    "        abstract += line + \"\\n\"\n",
    "\n",
    "    return abstract\n",
    "\n",
    "def extract_section(_dict: dict[str, ScannedText]) -> dict[str, str]:\n",
    "    dict_section: Dict[str, str] = {}\n",
    "\n",
    "    for key, value in _dict.items():\n",
    "\n",
    "        if value.index > -1:\n",
    "            abstract_lines = remove_white_spaces(value.index, value.text)\n",
    "            text = extract_section_lines(value.index, abstract_lines)\n",
    "        else:\n",
    "            text = \"Not found\"\n",
    "\n",
    "        dict_section[key] = text\n",
    "\n",
    "    return dict_section\n",
    "\n",
    "dict_abstracts = extract_section(abstracts)\n",
    "dict_introductions = extract_section(introductions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://api.crossref.org/works?query.author=Eli+%28Omid%29+David+and+Nathan+S.+Netanyahu+and+Lior+Wolf&query.bibliographic=DeepChess%3A+End-to-End+Deep+Neural+Network+for+Automatic+Learning+in+Chess\n",
      "https://api.crossref.org/works?query.author=Michael+George+and+Jonathan+Schaeffer&query.bibliographic=Chunking+for+Experience\n",
      "https://api.crossref.org/works?query.author=R.+Feldmann+and+P.+Mysliwietz+and+B.+Monien&query.bibliographic=A+Fully+Distributed+Chess+Program\n",
      "https://api.crossref.org/works?query.author=Joe+Condon+and+Ken+Thompson&query.bibliographic=Belle+Chess+Hardware\n",
      "https://api.crossref.org/works?query.author=Don+Beal&query.bibliographic=Intelligent+Systems%2C+Artificial+and+Human\n",
      "Error during request to CrossRef REST API: 'title'\n",
      "https://api.crossref.org/works?query.author=Paolo+Ciancarini+and+Gian+Piero+Favini&query.bibliographic=Plagiarism+detection+in+game-playing+software\n",
      "https://api.crossref.org/works?query.author=G.+Haworth+and+H.+M.+J.+F.+van+der+Heijden+and+E.+Bleicher&query.bibliographic=Zugzwangs+in+chess+studies\n",
      "https://api.crossref.org/works?query.author=Leroy+Panek&query.bibliographic=%22Maelzel%27s+Chess-Player%22%2C+Poe%27s+First+Detective+Mistake\n",
      "https://api.crossref.org/works?query.author=Isaac+Kamlish+and+Isaac+Bentata+Chocron+and+Nicholas+McCarthy&query.bibliographic=SentiMATE%3A+Learning+to+play+Chess+through+Natural+Language+Processing\n",
      "https://api.crossref.org/works?query.author=Brockington%2C+Mark&query.bibliographic=A+taxonomy+of+parallel+game-tree+search+algorithms\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import requests\n",
    "\n",
    "from difflib import SequenceMatcher\n",
    "from crossref.restful import Works\n",
    "\n",
    "def similar(title_extracted:str, title_crossref: str) -> float:\n",
    "    return SequenceMatcher(None, title_extracted, title_crossref).ratio()\n",
    "\n",
    "work = Works()\n",
    "\n",
    "def send_crossref_request(title: str, author: str) -> str:\n",
    "    url_request = work.query(bibliographic=title, author=author).url\n",
    "\n",
    "    # Attardare di un certo delay come definito dalla libreria per ogni richiesta successiva\n",
    "    time.sleep(2)\n",
    "    print(url_request)\n",
    "\n",
    "    try:\n",
    "        request = requests.get(url_request)\n",
    "\n",
    "        match request.status_code:\n",
    "            case 200:\n",
    "                response = request.json()\n",
    "\n",
    "                message = response.get(\"message\")\n",
    "                items = message.get(\"items\")\n",
    "\n",
    "                for item in items:\n",
    "                    if similar(title, item[\"title\"][0]) > 0.5:\n",
    "                        return item[\"DOI\"]\n",
    "                                                                \n",
    "                    continue\n",
    "            case 400:\n",
    "                raise Exception(\"Error during request to CrossRef REST API: Bad Request\")\n",
    "            case _:\n",
    "                raise Exception(\"Error during request to CrossRef REST API\")\n",
    "    except Exception as e:\n",
    "        print(\"Error during request to CrossRef REST API:\", e)\n",
    "\n",
    "for item in list_metadata:\n",
    "    if \"Not found\" not in (item.title, item.author):\n",
    "        doi =  send_crossref_request(item.title, item.author)\n",
    "\n",
    "        if doi is not None:\n",
    "            item.DOI = doi\n",
    "\n",
    "    item.abstract = dict_abstracts[item.path]\n",
    "    \n",
    "    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_json = []\n",
    "for metadata in list_metadata:\n",
    "    list_json.append(metadata.get_dict())\n",
    "\n",
    "with open(\"../json/metadata.json\", \"a\") as file:\n",
    "    json.dump(list_json, file, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'utilizzo di una __chain__, secondo le regole implementative espresse dalla libreria __langchain__, ha permesso l'estrazione del _titolo_ e dell'_autore_ per tutti i file non in possesso dei _metadati_ ricercati. Tuttavia, ciò ha garantito l'estrazione delle informazioni per un totale di 8 file su 10, pertanto un rapporto che va oltre alla media, si osservi il risultato ottenuto in seguito al _run all_ in __json/metadata.json__.\n",
    "\n",
    "Piccoli accorgimenti potrebbero essere utilizzati per i seguenti articoli, di cui non si ha il _DOI_:\n",
    "- 91FeldmannMysliwietzMonien\n",
    "- 07Beal\n",
    "- 19Kamlish\n",
    "\n",
    "Il file denominato _91FeldmannMysliwietzMonien_ presenta l'_identificativo digitale_ nella sezione __reference>unstructured__ della risposta ricevuta dalla _REST API_.\n",
    "L'ultimo riportato è stato cercato manualmente all'interno dell'API di _CrossRef_; infatti non è stata delineata una determinata persistenza dei dati, anzi l'articolo scientifico non compare tra quelli proposti."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
