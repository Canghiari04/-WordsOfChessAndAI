{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Obiettivo__\n",
    "\n",
    "Estrarre alcuni metadati relativi ai file PDF contenuti nella cartella _articles_, mediante l'impiego di apposite librerie per la gestione e manipolazione di file PDF.\n",
    "\n",
    "I dati estratti sono suddivisi in:\n",
    "- __DOI__, Digital Object Identifier, identificativo univoco di risorse digitali\n",
    "- __Title__, titolo dell'articolo / paper scientifico \n",
    "- __Author__, autore / autori partecipanti alla stesura dell'articolo preso in considerazione\n",
    "- __Abstract__, riassunto di un documento, privo di interpretazioni o critiche\n",
    "\n",
    "_DOI_ è l'acronimo di __Digital Object Identifier__, identificativo univoco di risorse digitali. Per poter apprendere tale informazione è necessario utilizzare la _REST API_ fornita da __CrossRef__; CrossRef è un'infrastuttura digitale dedita alle memorizzazione di tutti gli articoli posti in ambito accademico. Di seguito, si percepisce l'importanza del titolo e dell'autore per ogni PDF, affinchè interrogando la _Application Programming Interface_ sia possibile ottenere il DOI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_word = \"abstract\"\n",
    "pdf_path = \"../articles/\"\n",
    "crossref_url = \"https://api.crossref.org/works/10.1037/0003-066X.59.1.29/agency\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le tre classi riportate sono utilizzate rispettivamente per:\n",
    "- __ScannedText__, contiene sezioni del file PDF scannerizzato, dove _index_ rappresenta l'indice della linea in cui compare la _key word_ ricercata, mentre _text_ è una variabile _str_ utilizzata per memorizzare la scansione da immagine a stringa effettuata tramite _pytesseract_\n",
    "- __Metadata__, ciascun oggetto istanziato della classe rappresenta i metadati ottenuti di ogni singolo file presente all'interno della cartella _articles_\n",
    "- __AgentExtractor__, implementata per realizzare una __chain__ secondo le direttive della libreria __langchain__. Utilizzata per poter estrarre il _titolo_ e gli _autori_ degli articoli privi di _metadati_ già presenti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScannedText:\n",
    "    def __init__(self, start_index, end_index, text):\n",
    "        self.start_index = start_index\n",
    "        self.end_index = end_index\n",
    "        self.text = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "class ScannedSection:\n",
    "    def __init__(self, path, abstract, introduction):\n",
    "        self.path = path\n",
    "        self.abstract = abstract\n",
    "        self.introduction = introduction\n",
    "\n",
    "    def get_dict(self) -> Dict[str, Dict[str, str]]:\n",
    "        return {\n",
    "            self.path: {\n",
    "                \"Abstract\": self.abstract,\n",
    "                \"Introduction\": self.introduction\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metadata:\n",
    "    def __init__(self, DOI=None, path=None, title=None, author=None, abstract=None):\n",
    "        self.DOI = DOI\n",
    "        self.path = path\n",
    "        self.title = title\n",
    "        self.author = author\n",
    "        self.abstract = abstract\n",
    "\n",
    "    def get_dict(self) -> Dict[str, Dict[str, str]]:\n",
    "        return {\n",
    "            self.path: {\n",
    "                \"DOI\": self.DOI,\n",
    "                \"Title\": self.title,\n",
    "                \"Author\": self.author,\n",
    "                \"Abstract\": self.abstract\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "class AgentExtractor:\n",
    "    def __init__(self):\n",
    "        system_message = \"\"\"\n",
    "            You are an assistant in charge to extract the title and the authors' list of scientific paper.\n",
    "\n",
    "            You must extract at least the title and the authors, any other information is not requested. Return all the\n",
    "            fields required like an unique string; all the fields must be separated by a comma.\n",
    "\n",
    "            Example of an extraction:\n",
    "            Title, First Author, Second Author, ..., Last Author\n",
    "        \"\"\"\n",
    "\n",
    "        prompt = ChatPromptTemplate(\n",
    "            [\n",
    "                (\"system\", system_message),\n",
    "                (\"human\", \"Scientific paper, which will be used to extract the title and the authors' list. \\n {text}\")\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        llm = ChatOpenAI(\n",
    "            model=\"gpt-4o\",\n",
    "            api_key=os.getenv(\"OPENAI_KEY\"),\n",
    "            temperature=0\n",
    "        )\n",
    "\n",
    "        self.agent = prompt | llm\n",
    "\n",
    "    def get_agent(self):\n",
    "        return self.agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La pipeline è composta da due fasi principali:\n",
    "- _Estrazione_ dei metadati\n",
    "- _Elaborazione_ dei metadati ottenuti\n",
    "\n",
    "Le librerie utilizzate per l'acquisizione dei metadati si fondano a loro volta sulla libreria __pdfMiner__, pertanto riescono ad estrapolare le informazioni circoscritte dagli stessi file. Perciò tutte le librerie trovate si equivalgono."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def get_path_pdf_files(pdf_path: str) -> List[str]:\n",
    "    path_files = []\n",
    "\n",
    "    for path in os.listdir(pdf_path):\n",
    "        path_files.append(pdf_path + path)\n",
    "    \n",
    "    return path_files\n",
    "\n",
    "path_pdf_files = get_path_pdf_files(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pytesseract\n",
    "\n",
    "from pdf2image import convert_from_path\n",
    "\n",
    "def convert_file_to_images(path: str) -> list:\n",
    "    try:\n",
    "        return convert_from_path(path)[:3]\n",
    "    except Exception as e:\n",
    "        print(\"Error during conversion from file to image:\", e)\n",
    "\n",
    "def scan_file_to_text(paths: list[str]) -> dict[str, str]:\n",
    "    dict_texts: Dict[str, str] = {}\n",
    "\n",
    "    for path in paths:\n",
    "        images = convert_file_to_images(path)\n",
    "\n",
    "        text = \"\"\n",
    "        try:\n",
    "            for image in images:\n",
    "                text = text + pytesseract.image_to_string(image)\n",
    "\n",
    "            dict_texts[path] = text\n",
    "        except Exception as e:\n",
    "            print(\"Error during conversion from image to text:\", e) \n",
    "        \n",
    "    return dict_texts\n",
    "\n",
    "dict_scanned_text = scan_file_to_text(path_pdf_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_word_index(expression: str, text: str) -> int:\n",
    "    count_line = 0\n",
    "\n",
    "    lines = text.splitlines()\n",
    "    for line in lines:\n",
    "        if re.search(expression, line.lower()) is not None:\n",
    "            return count_line\n",
    "        \n",
    "        count_line += 1\n",
    "    \n",
    "    return -1\n",
    "\n",
    "def detect_target_line(start_word: str, end_word: str, dict: dict[str, str]) -> dict[str, ScannedText]:\n",
    "    dict_lines: Dict[str, ScannedText] = {}\n",
    "\n",
    "    for key, value in dict.items():\n",
    "        dict_lines[key] = ScannedText(get_target_word_index(start_word, value.lower()), get_target_word_index(end_word, value.lower()), value)\n",
    "\n",
    "    return dict_lines\n",
    "\n",
    "abstracts = detect_target_line(r\"^abstract\", r\"1?\\.?\\s* [Ii]ntroduction\", dict_scanned_text)\n",
    "introductions = detect_target_line(r\"1?\\.?\\s* [Ii]ntroduction\", r\"^2\", dict_scanned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 2 65536 (offset 0)\n",
      "Ignoring wrong pointing object 16 65536 (offset 0)\n",
      "Ignoring wrong pointing object 49 65536 (offset 0)\n",
      "Ignoring wrong pointing object 59 65536 (offset 0)\n",
      "Ignoring wrong pointing object 63 65536 (offset 0)\n",
      "Ignoring wrong pointing object 72 65536 (offset 0)\n",
      "Ignoring wrong pointing object 76 65536 (offset 0)\n",
      "Ignoring wrong pointing object 86 65536 (offset 0)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pymupdf\n",
    "import pdfplumber\n",
    "\n",
    "from pypdf import PdfReader\n",
    "\n",
    "def get_title_from_dicts(pdfreader: str, pymupdf: dict[str, str], pdfplumber: dict[str, str]) -> str:\n",
    "    return (pdfreader or pymupdf.get(\"title\") or pdfplumber.get(\"title\") or \"Not found\")\n",
    "\n",
    "def get_author_from_dicts(pdfreader: str, pymupdf: dict[str, str], pdfplumber: dict[str, str]) -> str:\n",
    "    return (pdfreader or pymupdf.get(\"author\") or pdfplumber.get(\"author\") or \"Not found\")\n",
    "\n",
    "def extract_title_and_author(i: int, len: int, paths: list[str], list_metadata: List[Metadata] = []) -> List[Metadata]:\n",
    "    if len == 0:\n",
    "        return list_metadata\n",
    "    else:\n",
    "        _pdfreader = PdfReader(paths[i]).metadata\n",
    "        _pymupdf = pymupdf.open(paths[i]).metadata\n",
    "        _pdfplumber = pdfplumber.open(paths[i]).metadata\n",
    "\n",
    "        title = get_title_from_dicts(_pdfreader.title, _pymupdf, _pdfplumber)\n",
    "        author = get_author_from_dicts(_pdfreader.author, _pymupdf, _pdfplumber)\n",
    "\n",
    "        list_metadata.append(Metadata(path=paths[i], title=title, author=author))\n",
    "\n",
    "        extract_title_and_author(i + 1, len - 1, paths)\n",
    "        return list_metadata\n",
    "\n",
    "list_metadata = extract_title_and_author(0, len(path_pdf_files), path_pdf_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Piccolo catalogo di articoli a cui manca anche il titolo e autore:\n",
    "- 09CiancariniFavini\n",
    "- 16DavidNetanyahuWolf\n",
    "- 19Kamlish\n",
    "- ICGA_J_34_2_HHB_Zugzwangs_in_Chess_Studies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_authors_name(i: int, len:int, authors: list[str], field = \"\") -> str:\n",
    "    if len == 1:\n",
    "        return field + authors[i]\n",
    "    else:\n",
    "        return get_authors_name(i + 1, len - 1, authors, field + authors[i] + \" and \")\n",
    "    \n",
    "extractor = AgentExtractor()\n",
    "agent_extractor = extractor.get_agent()\n",
    "\n",
    "for metadata in list_metadata:\n",
    "    if \"Not found\" in (metadata.title, metadata.author):\n",
    "        metadata.title = \"\"\n",
    "        metadata.author = \"\"\n",
    "\n",
    "        answer = agent_extractor.invoke(dict_scanned_text.get(metadata.path)).content\n",
    "        fields = answer.split(\", \")\n",
    "\n",
    "        metadata.title = fields[0]\n",
    "        metadata.author = get_authors_name(0, len(fields[1:]), fields[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "\n",
    "def extract_section_lines(start_index: int, end_index: int, text: str) -> str:\n",
    "    abstract = \"\"\n",
    "    lines = text.splitlines()\n",
    "\n",
    "    for line in islice(lines, start_index, end_index):\n",
    "        abstract += line + \"\\n\"\n",
    "\n",
    "    return abstract\n",
    "\n",
    "def extract_section(_dict: dict[str, ScannedText]) -> dict[str, str]:\n",
    "    dict_section: Dict[str, str] = {}\n",
    "\n",
    "    for key, value in _dict.items():\n",
    "        if value.start_index > -1 and value.end_index > -1:\n",
    "            text = extract_section_lines(value.start_index, value.end_index, value.text)\n",
    "        else:\n",
    "            text = \"Not found\"\n",
    "\n",
    "        dict_section[key] = text\n",
    "\n",
    "    return dict_section\n",
    "\n",
    "dict_abstracts = extract_section(abstracts)\n",
    "dict_introductions = extract_section(introductions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I metodi sviluppati, per l'estrazione della _Abstract Section_ oppure della _Introduction Section_, presentano alcune problematiche soprattutto in ottica del formato del file analizzato. Infatti, documenti che dispongono il testo in due colonne distinte sono più difficili da convertire in un formato idoneo alla manipolazione testuale, ad esempio ciò avviene per il PDF _09CiancariniFavini_. Tuttavia, è presente un ulteriore file che possiede un formato simile, ossia _19Kamlish_, da cui è possibile estrarre integralmente le sezioni citate.\n",
    "\n",
    "Pertanto, possono essere assecondati due approcci sostitutivi a quanto descritto, tra cui:\n",
    "- __CrossRef__, interrogare la Rest API per ottenere l'_Abstract_ tramite il _Digital Object Identifier_\n",
    "- __OCR__, implementazione di un differente _Optical Character Recognition_, oltre alla libreria _pytesseract_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://api.crossref.org/works?query.author=Eli+%28Omid%29+David+and+Nathan+S.+Netanyahu+and+Lior+Wolf&query.bibliographic=DeepChess%3A+End-to-End+Deep+Neural+Network+for+Automatic+Learning+in+Chess\n",
      "https://api.crossref.org/works?query.author=Michael+George+and+Jonathan+Schaeffer&query.bibliographic=Chunking+for+Experience\n",
      "https://api.crossref.org/works?query.author=R.+Feldmann+and+P.+Mysliwietz+and+B.+Monien&query.bibliographic=A+Fully+Distributed+Chess+Program\n",
      "https://api.crossref.org/works?query.author=Joe+Condon+and+Ken+Thompson&query.bibliographic=Belle+Chess+Hardware\n",
      "https://api.crossref.org/works?query.author=Don+Beal&query.bibliographic=Intelligent+Systems%2C+Artificial+and+Human\n",
      "Error during request to CrossRef REST API: 'title'\n",
      "https://api.crossref.org/works?query.author=Paolo+Ciancarini+and+Gian+Piero+Favini&query.bibliographic=Plagiarism+detection+in+game-playing+software\n",
      "https://api.crossref.org/works?query.author=G.+Haworth+and+H.+M.+J.+F.+van+der+Heijden+and+E.+Bleicher&query.bibliographic=Zugzwangs+in+chess+studies\n",
      "https://api.crossref.org/works?query.author=Leroy+Panek&query.bibliographic=%22Maelzel%27s+Chess-Player%22%2C+Poe%27s+First+Detective+Mistake\n",
      "https://api.crossref.org/works?query.author=Isaac+Kamlish+and+Isaac+Bentata+Chocron+and+Nicholas+McCarthy&query.bibliographic=SentiMATE%3A+Learning+to+play+Chess+through+Natural+Language+Processing\n",
      "https://api.crossref.org/works?query.author=Brockington%2C+Mark&query.bibliographic=A+taxonomy+of+parallel+game-tree+search+algorithms\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import requests\n",
    "\n",
    "from crossref.restful import Works\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "def similar(title_extracted:str, title_crossref: str) -> float:\n",
    "    return SequenceMatcher(None, title_extracted, title_crossref).ratio()\n",
    "\n",
    "work = Works()\n",
    "\n",
    "def send_crossref_request(title: str, author: str) -> str:\n",
    "    url_request = work.query(bibliographic=title, author=author).url\n",
    "    print(url_request)\n",
    "\n",
    "    # Delay the request to CrossRef Rest API how asked by the same library\n",
    "    time.sleep(2)\n",
    "\n",
    "    try:\n",
    "        request = requests.get(url_request)\n",
    "\n",
    "        match request.status_code:\n",
    "            case 200:\n",
    "                response = request.json()\n",
    "\n",
    "                message = response.get(\"message\")\n",
    "                items = message.get(\"items\")\n",
    "\n",
    "                for item in items:\n",
    "                    if similar(title, item[\"title\"][0]) > 0.5:\n",
    "                        return item[\"DOI\"]\n",
    "                                                                \n",
    "                    continue\n",
    "            case 400:\n",
    "                raise Exception(\"Error during request to CrossRef REST API: Bad Request\")\n",
    "            case _:\n",
    "                raise Exception(\"Error during request to CrossRef REST API\")\n",
    "    except Exception as e:\n",
    "        print(\"Error during request to CrossRef REST API:\", e)\n",
    "\n",
    "for item in list_metadata:\n",
    "    if \"Not found\" not in (item.title, item.author):\n",
    "        doi =  send_crossref_request(item.title, item.author)\n",
    "\n",
    "        if doi is not None:\n",
    "            item.DOI = doi\n",
    "\n",
    "    item.abstract = dict_abstracts[item.path]\n",
    "    \n",
    "    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_scanned = []\n",
    "for key in dict_abstracts.keys():\n",
    "    list_scanned.append(ScannedSection(key, dict_abstracts[key], dict_introductions[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Tramite la funzione _filter_, associata al metodo _query_ fornita dalla libreria __crossref__, è possibile recuperare l'_Abstract_ degli articoli presenti all'interno dell'API. Tuttavia, è stato notato che l'impiego del filtro comporta ad una risposta completamente differente rispetto alla casistica in cui sia assente. Di seguito, è presentato lo snippet di codice implementato:\n",
    "\n",
    "<div align=\"center\">\n",
    "    <p><b>url_request = work.query(bibliographic=title, author=author).filter(has_abstract=true).url</b></p>\n",
    "</div>\n",
    "\n",
    "Gli __url__ sottostanti non ricadono nello stesso dominio individuato nelle precedenti interrogazioni all'API durante la fase di acquisizione del _DOI_. Di seguito è stato implementato un ulteriore approccio, in cui viene ricavato il _Uniform Resource Locator_ attraverso la combinazione dell'identificativo digitale e dell'estensione _.xml_, come definito dagli stessi manuntentori dell'API. Tuttavia, anche in questa casistica non è mai riportato il _tag </abstract/>_.\n",
    "\n",
    "Unico approccio risolutivo possibile potrebbe consistere in una maggiore documentazione relativa agli _Abstract_ contenuti all'interno di _CrossRef_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://api.crossref.org/works?filter=has-abstract%3Atrue&query.author=Joe+Condon+and+Ken+Thompson&query.bibliographic=Belle+Chess+Hardware\n",
      "https://api.crossref.org/works?filter=has-abstract%3Atrue&query.author=Leroy+Panek&query.bibliographic=%22Maelzel%27s+Chess-Player%22%2C+Poe%27s+First+Detective+Mistake\n",
      "https://api.crossref.org/works?filter=has-abstract%3Atrue&query.author=Isaac+Kamlish+and+Isaac+Bentata+Chocron+and+Nicholas+McCarthy&query.bibliographic=SentiMATE%3A+Learning+to+play+Chess+through+Natural+Language+Processing\n",
      "https://api.crossref.org/works?filter=has-abstract%3Atrue&query.author=Brockington%2C+Mark&query.bibliographic=A+taxonomy+of+parallel+game-tree+search+algorithms\n",
      "\n",
      "\n",
      "https://api.crossref.org/works/10.1007/978-1-4757-1968-0_28.xml\n",
      "https://api.crossref.org/works/10.2307/2924872.xml\n",
      "https://api.crossref.org/works/10.1007/978-94-009-5044-3_16.xml\n",
      "https://api.crossref.org/works/10.3233/icg-1996-19303.xml\n"
     ]
    }
   ],
   "source": [
    "for item in list_metadata:\n",
    "    if item.DOI is not None and \"Not found\" in item.abstract:\n",
    "        url_request = work.query(bibliographic=item.title, author=item.author).filter(has_abstract=\"true\").url\n",
    "        print(url_request)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "for item in list_metadata:\n",
    "    if item.DOI is not None and \"Not found\" in item.abstract:\n",
    "        url_request = work.query().url + \"/\" + item.DOI + \".xml\"\n",
    "        print(url_request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_metadata_json = []\n",
    "for metadata in list_metadata:\n",
    "    list_metadata_json.append(metadata.get_dict())\n",
    "\n",
    "list_scanned_json = []\n",
    "for introduction in list_scanned:\n",
    "    list_scanned_json.append(introduction.get_dict())\n",
    "\n",
    "with open(\"../json/metadata.json\", \"a\") as file:\n",
    "    json.dump(list_metadata_json, file, indent=2)\n",
    "\n",
    "with open(\"../json/introduction.json\", \"a\") as file:\n",
    "    json.dump(list_scanned_json, file, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'utilizzo di una __chain__, secondo le regole implementative espresse dalla libreria __langchain__, ha permesso l'estrazione del _titolo_ e dell'_autore_ per tutti i file non in possesso dei _metadati_ ricercati. Tuttavia, ciò ha garantito l'estrazione delle informazioni per un totale di 8 file su 10, pertanto un rapporto che va oltre alla media, si osservi il risultato ottenuto in seguito al _run all_ in __json/metadata.json__.\n",
    "\n",
    "Migliorie possono essere adottate per quanto concerne la _Regular Expression_ utilizzata per estrapolare la _Introduction Section_.\n",
    "\n",
    "Piccoli accorgimenti potrebbero essere utilizzati per i seguenti articoli, di cui non si ha il _DOI_:\n",
    "- _91FeldmannMysliwietzMonien_\n",
    "- _07Beal_\n",
    "- _19Kamlish_\n",
    "\n",
    "Il file denominato _91FeldmannMysliwietzMonien_ presenta l'_identificativo digitale_ nella sezione __reference>unstructured__ della risposta ricevuta dalla _REST API_.\n",
    "Gli ultimi due riportati sono stati cercati manualmente all'interno dell'API di _CrossRef_; infatti non è stata delineata una determinata persistenza dei dati, anzi gli articoli scientifici non compaiono tra quelli proposti."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
